}
#create SampleID column to left join with taxa and metadata and then melt to longform
seqmat$SampleID <- rownames(seqmat)
seqmat_melt <- melt(seqmat)
#name columns for joining
colnames(seqmat_melt)[2] <- "OTU"
colnames(seqmat_melt)[3] <- "Abundance"
#create column for joining
taxa <- as.data.frame(asv_list$taxmat)
taxa$OTU <- rownames(taxa)
#create h_clust object
h_clust <- asv_list$h_clust
#left join to meta, then to taxa, then to cluster
asv_melt <- left_join(seqmat_melt, meta, "SampleID")
asv_melt$OTU <- as.character(asv_melt$OTU)
asv_melt <- left_join(asv_melt, taxa, "OTU")
#incorporate cluster vector if it exists
if (!is.null(asv_list$h_clust)){
h_clust <- asv_list$h_clust
h_clust$OTU <- as.character(h_clust$OTU)
asv_melt <- left_join(asv_melt, h_clust, "OTU")
#convert cluster to character for downstream graphing functions
asv_melt$cluster <- as.character(asv_melt$cluster)
}
#convert time variable to a factor for downstream graphing functions
asv_melt[,time_var] <- as.factor(asv_melt[,time_var])
return(asv_melt)
}
melted_asv_list <- melt_asv_list(asv_list = asv_list_clean, sam_var = "Sample", time_var = "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list, "Timepoint")
asv_list_clean <- asv_clustr(asv_list = asv_list_clean, td = "both",k = 3)
asv_list_clean <- compute_pd(asv_list = asv_list_clean, sam_var = "Sample", time_var = "Timepoint",independent_var = "cage")
test_clean <- compare_pd(asv_list = asv_list_clean, calc_CI = TRUE, boot_k = 500, groups = c("C", "D"))
test_clean$mean_difference <- test_clean$term_means - test_clean$base_mean
ggplot(test_clean, aes(x = mean_difference, y = term, color = term, fill = term)) + facet_wrap(.~ cluster, scales = "free") + geom_line() + geom_pointrange(data = test_clean, aes(xmin = mean_difference - lower, xmax = mean_difference + upper), width = 2) + geom_vline(aes(xintercept = 0))
melted_asv_list <- melt_asv_list(asv_list = asv_list_clean, sam_var = "Sample", time_var = "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list, "Timepoint")
#' @export
#' @param asv_list An asv_list.
#' @param sam_var A string. The metadata variable for samples.
#' @param time_var A string. The metadata variable for timepoints.
#' @importFrom methods isClass
#' @return An asv_list.
#' @examples
#'
#' asv_list <- calc_td(asv_list = asv_list,  sam_var = "Sample", time_var = 'Timepoint')
#'
calc_td <- function(asv_list, sam_var, time_var){
if (!isClass(asv_list, Class = c("list", "ASVclustr"))){
stop("asv_list must be an object of class list and ASVclustr")
}
#define variables
seqmat_td <- as.data.frame(asv_list$seqmat)
meta <- asv_list$meta
#arrange by timepoint in ascending order, convert into a list of data.frames
#where each df is one sample
seqmat_td_list <- order_seqmat(seqmat = seqmat_td, meta = meta,
sam_var = sam_var, time_var = time_var)
#remove character vectors from each dataframe
seqmat_td_list <- lapply(seqmat_td_list, function(x)
x[,!(names(x) %in% c("SampleID","sample"))])
#This function will compute the time derivative for every sequential time interval for each
#ASV and will be iterated across each data frame in the list using lapply
#f'(x) = (f(x+h) - f(x))/h
#this function can take samples with unequal length time vectors
#diff iterates the difference down the rows within a column
compute_derivatives <- function(x, time_var){
#coerce the dataframe to a matrix within diff to index
#all ASV columns
d <- diff(as.matrix(x[!(names(x) %in% c(time_var))]))/diff(x[,ncol(x)])
return(d)
}
#iterate compute derivatives across each data frame in the list.
seqmat_derivatives_list <- lapply(seqmat_td_list, compute_derivatives,
time_var = time_var)
#change the rownames in seqtab_derivatives_list to avoid repeated rownames with seqmat
seqmat_derivatives_list <- lapply(seqmat_derivatives_list,
function(x){ rownames(x) <- paste0(rownames(x), ".","TD"); x})
#rbind the two lists together, element by element
seqmat_derivatives <- do.call("rbind", seqmat_derivatives_list)
#remove timepoint_var column in each dataframe of seqtab_order_list
seqmat_derivatives <- seqmat_derivatives[,colnames(seqmat_derivatives)!= "timepoint"]
#convert na values to 0
seqmat_derivatives <- apply(seqmat_derivatives, 2, function (x) ifelse(is.na(x), 0, x))
#add derivatives to asv list
asv_list$seqmat_td <- seqmat_derivatives
return(asv_list)
}
#' plot(clustr_dendro, labels = FALSE)
#' rect.hclust(tree = clustr_dendro, border ="red", k=4)
#'
#' asv_list <- asv_clustr(asv_list = asv_list,  k=4)
#'
#' asv_list <- asv_clustr(asv_list = asv_list,  k=4, td = "both")
#'
#' asv_list <- asv_clustr(asv_list = asv_list,  k=4, td = "td_only")
#'
#'
asv_clustr <- function(asv_list, agg_method = "ward.D2", td = "none", k = NULL){
if (!isClass(asv_list, Class = c("list", "ASVclustr"))){
stop("asv_list must be of classes list and ASVclustr")
}
switch(td,
both = {
if (is.null(asv_list$seqmat_td)){
stop("asv_list must have a seqmat_td element to use parameter td = 'both'")
}
seqmat <- rbind(asv_list$seqmat, asv_list$seqmat_td)
},
td_only = {
if (is.null(asv_list$seqmat_td)){
stop("asv_list must have a seqmat_td element to use parameter td = 'td_only'")
}
seqmat <- asv_list$seqmat_td
},
none = seqmat <- asv_list$seqmat,
stop("input must be one of both, td_only, or none")
)
#compute cosine similarity and angular distance
cosine_sim <- dist.matrix(M = seqmat, convert = TRUE, byrow = FALSE)
#perform hierarchical clustering
if (is.null(k)){
h_clust <- hclust(d = as.dist(cosine_sim),
method = agg_method)
return(h_clust)
} else if (!is.null(k)){
h_clust <- as.data.frame(cutree(hclust(d = as.dist(cosine_sim),
method = agg_method),k = k))
#format cluster dataframe
colnames(h_clust)[1] <- "cluster"
h_clust$OTU <- rownames(h_clust)
asv_list$h_clust <- h_clust
return(asv_list)
}
}
asv_list_clean <- asv_clustr(asv_list = asv_list_clean, td = "both",k = 4)
asv_list_clean <- calc_td(asv_list = asv_list_clean, "Sample", "Timepoint")
asv_list_clean <- asv_clustr(asv_list = asv_list_clean, td = "both",k = 4)
melted_asv_list <- melt_asv_list(asv_list = asv_list_clean, sam_var = "Sample", time_var = "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list, "Timepoint")
asv_list_clean_cageC <- remove_samples(asv_list = asv_list_clean, variable = "cage", variable_level = "D")
melted_asv_list-cageC <- melt_asv_list(asv_list = asv_list_clean_cageC, sam_var = "Sample", time_var = "Timepoint")
melted_asv_list_cageC <- melt_asv_list(asv_list = asv_list_clean_cageC, sam_var = "Sample", time_var = "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list_cageC, "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list, "Timepoint")
asv_list_clean_cageD <- remove_samples(asv_list = asv_list_clean, variable = "cage", variable_level = "C")
melted_asv_list_cageD <- melt_asv_list(asv_list = asv_list_clean_cageD, sam_var = "Sample", time_var = "Timepoint")
cluster_means_bar(melted_asv_list = melted_asv_list_cageD, "Timepoint")
compare_pd <- function(asv_list, batch=FALSE, calc_CI=FALSE, boot_k=NULL,groups=NULL){
#set object
seqmat_integrals_list <- asv_list$seqmat_integrals
#function to remove ASVs that are zero across all samples in a variable level
remove_zero_ASVs <- function(x,variable){
#create variable for whether or not an ASV is present in a sample
x$non_zero <- ifelse(x$value > 0, 1, 0)
#for the gamma distribution to model the continuous response variable of ASV population
#during a timecourse (quantified by the integral of the abundance curve), we remove samples which
#have an integral of zero
non_zero_only <- subset(x, non_zero==1)
non_zero_only[] <- lapply(non_zero_only, function(x) if(is.factor(x)) factor(x) else x)
#this control flow step sets dataframes within a list to NULL if one factor level is all 0 values
if (length(levels(non_zero_only[,variable])) <= length(levels(x[,variable])) - 1) {
return(NULL)
} else if (length(levels(non_zero_only[,variable])) == length(levels(x[,variable]))){
return(x)
}
}
#remove ASVs where there is a variable level that contains all 0 values for the integral if comparison is chosen
if (!is.null(groups)){
if (batch == FALSE){
seqmat_integrals_list <- lapply(seqmat_integrals_list, remove_zero_ASVs, variable = "independent_var")
} else if (batch == TRUE){
seqmat_integrals_list <- lapply(seqmat_integrals_list, remove_zero_ASVs, variable = "independent_var")
seqmat_integrals_list <- lapply(seqmat_integrals_list, remove_zero_ASVs, variable = "batch")
}
seqmat_integrals_list <- seqmat_integrals_list[lapply(seqmat_integrals_list, is.null)==FALSE]
} else if (is.null(groups)){
seqmat_integrals_list <- lapply(seqmat_integrals_list, function(x) cbind(x, non_zero = ifelse(x$value > 0, 1, 0)))
}
#set variable names for bootstrap repetition check function
if (calc_CI == TRUE){
if (is.null(groups)){
variable <- NULL
} else if (!is.null(groups)){
if (batch == FALSE){
variable <- "independent_var"
} else if (batch == TRUE){
variable <- "batch"
}
}
boot_k <- boot_k
#returns TRUE if sample size is sufficient for number of selected bootstraps, FALSE if not
validate_reps <- function(x,variable){
#remove rows which are zero
x <- x[x$value > 0,]
if (is.null(variable)){
#group by variable, then tally sample size by group
x <- x %>%
tally()
} else if (!is.null(variable)){
#set column name for grouping
colnames(x)[colnames(x) == variable] <- "variable"
#group by variable, then tally sample size by group
x <- x %>%
group_by(variable) %>%
tally()
}
#return logical of whether sample size is too low for R bootstraps
x <- boot_k < choose(2*x$n - 1, x$n)
x
}
#iterate on seqmat_integrals
boot_check_list <- lapply(seqmat_integrals_list, validate_reps, variable=variable)
boot_check <- boot_check_list[lapply(boot_check_list, function(x) any(x == FALSE))==TRUE]
if (length(boot_check) > 0){
message("One or more response variables have an insufficient sample size for selected number of bootstraps.
Returning a vector of response variables with insufficient sample size")
return(asv_vector <- names(boot_check))
} else if (length(boot_check) == 0){
}
}
#flow control for creating a model formula
if (is.null(groups)){
bin_model <- non_zero ~ 1
gamma_model <- value ~ 1
} else if (!is.null(groups)){
if (batch == FALSE){
bin_model <- non_zero ~ independent_var
gamma_model <- value ~ independent_var
} else if (batch == TRUE){
bin_model <- non_zero ~ independent_var + batch
gamma_model <- value ~ independent_var + batch
}
}
#if a comparison vector is provided subset the variable to those two levels
if (is.null(groups)){
seqmat_integrals_list
} else if (!is.null(groups)){
seqmat_integrals_list <- lapply(seqmat_integrals_list, function(x) subset(x, independent_var %in% groups))
#reorder the factor in the order provided in the comparison vector
seqmat_integrals_list <- lapply(seqmat_integrals_list, function (x){ x$independent_var <- factor(x$independent_var, levels = groups); x})
}
#create a function for the model to iterate across the list
bin_gamma_hurdle <- function(x){
#fit a binomial logistic regression with the binary presence or absense as the response
# to predict the probability of an ASV being present during the timecourse
bin_mod <- glm(formula = bin_model, data = x, family = binomial(link = logit))
#fit a gamma GLM to the integral values
gamma_mod <- glm(formula = gamma_model, data = subset(x, non_zero==1),
family = Gamma(link = "log"))
#extract binomial model coefficients
bin_coef <- tidy(bin_mod)
bin_coef$model <- "binomial"
#extract gamma coefficients
gamma_coef <- tidy(gamma_mod)
gamma_coef$model <- "gamma"
mod <- rbind(gamma_coef, bin_coef)
#the probabiliy of the NULL hypothesis where ASV integrals are the same between groups is dependent on the probability of the
#NULL hypothesis that groups do not have an effect on the presence or absence of an ASV, so the p value from the
#binomial model is multiplied by the p value of the gamma model
combined_pval <- tibble(p.value = bin_coef$p.value * gamma_coef$p.value, model = "hurdle",
term = gamma_coef$term, estimate = NA, std.error = NA, statistic = NA)
#combine p_values and OTU into a data.frame
mod <- rbind(mod, combined_pval)
}
compute_CI <- function(x, i){
x <- x[i, ]
#fit a binomial logistic regression with the binary presence or absense as the response
# to predict the probability of an ASV being present during the timecourse
bin_mod <- glm(formula = bin_model, data = x, family = binomial(link = logit))
#fit a gamma GLM to the integral values
gamma_mod <- glm(formula = gamma_model, data = subset(x, non_zero==1),
family = Gamma(link = "log"))
#compute a CI on the model intercept, i.e. across all samples. useful for measuring dispersion
if (is.null(variable)){
#tidy the models into tibbles. the only observation in this case is the intercept coefficients
bin_coef <- tidy(bin_mod)
gamma_coef <- tidy(gamma_mod)
#if a comparison between groups is being performed, remove the intercept coefficients to compute CI on desired model term
} else if (!is.null(variable)){
#tidy the models into tibbles and remove intercept observations
bin_coef <- tidy(bin_mod)
#bin_coef <- bin_coef[-1,]
gamma_coef <- tidy(gamma_mod)
#gamma_coef <- gamma_coef[-1,]
}
#return model estimates to original scale. binomial model used a logit link so take the inverse
#gamma model used a log link so take the exponent.
#take the sum of the logarithm of both estimate coefficients to compute the product. this is the statistic being bootstrapped
bin_est <- plogis(bin_coef$estimate)
gamma_est <- exp(gamma_coef$estimate)
exp(log(bin_est) + log(gamma_est))
}
#iterate model across the list
mod_list <- lapply(seqmat_integrals_list, bin_gamma_hurdle)
#determine if we are comparing OTU means or cluster means
cluster_or_OTU <- lapply(seqmat_integrals_list, function(x) colnames(x)=="cluster")
cluster_or_OTU <- do.call("rbind", cluster_or_OTU)
#set a variable to join the model dataframes and CI dataframes based on whether it is clusters or OTUs
if (any(cluster_or_OTU==TRUE)==TRUE){
name_variable <- "cluster"
} else if (any(cluster_or_OTU==TRUE)==FALSE){
name_variable <- "OTU"
}
#create list of OTU or cluster names to eventually join the mod list to the CI list
names_df <- data.frame(names(mod_list))
colnames(names_df)[1] <- name_variable
names_list <- split(names_df, f = as.factor(names_df[,name_variable]))
#combine names to mod list dataframes and then melt into longform. Then rename columns in each dataframe
mod_list <- lapply(mod_list, `row.names<-`, NULL)
names_list <- lapply(names_list, `row.names<-`, NULL)
mod_list <- Map(f = cbind, mod_list, names_list)
mod_list <- lapply(mod_list, function(x){ x$model_term <- paste(x$model, "_", x$term) ; x})
#compute an adjusted p value for FDR for each model p value
#compute padj for binomial model
mod_list_binomial <- lapply(mod_list, function(x) subset(x, model=="binomial"))
mod_list_binomial <- do.call("rbind", mod_list_binomial)
mod_list_binomial <- split(mod_list_binomial, f = as.factor(mod_list_binomial$term))
mod_list_binomial <- lapply(mod_list_binomial, function(x) {x$padj <- p.adjust(p = x$p.value, method = "BH"); x})
mod_list_binomial <- do.call("rbind", mod_list_binomial)
mod_list_binomial$term <- rownames(mod_list_binomial)
#compute padj for gamma model
mod_list_gamma <- lapply(mod_list, function(x) subset(x, model=="gamma"))
mod_list_gamma <- do.call("rbind", mod_list_gamma)
mod_list_gamma <- split(mod_list_gamma, f = as.factor(mod_list_gamma$term))
mod_list_gamma <- lapply(mod_list_gamma, function(x) {x$padj <- p.adjust(p = x$p.value, method = "BH"); x})
mod_list_gamma <- do.call("rbind", mod_list_gamma)
mod_list_gamma$term <- rownames(mod_list_gamma)
#compute padj for hurdle
mod_list_hurdle <- lapply(mod_list, function(x) subset(x, model=="hurdle"))
mod_list_hurdle <- do.call("rbind", mod_list_hurdle)
mod_list_hurdle <- split(mod_list_hurdle, f = as.factor(mod_list_hurdle$term))
mod_list_hurdle <- lapply(mod_list_hurdle, function(x) {x$padj <- p.adjust(p = x$p.value, method = "BH"); x})
mod_list_hurdle <- do.call("rbind", mod_list_hurdle)
mod_list_hurdle$term <- rownames(mod_list_hurdle)
#row bind each of the padj dataframes into a single dataframe
mod_list_padj <- rbind(mod_list_binomial, mod_list_gamma, mod_list_hurdle)
mod_list_padj$term <- gsub(mod_list_padj$term, pattern = "\\..*", replacement = "")
mod_list_padj$model_term <- paste(mod_list_padj$model, "_", mod_list_padj$term)
#remove uneccessary columns and then split into a list by either cluster or OTU
mod_list_padj <- mod_list_padj[,c("model", name_variable, "padj", "term", "model_term")]
mod_list_padj <- split(mod_list_padj, f = as.factor(mod_list_padj[,name_variable]))
#remove cluster/OTU column and then match the adjusted p values to the corresponding data.frames in mod_list
mod_list_padj <- lapply(mod_list_padj, function(x) x[!names(x) %in% c(name_variable, "term", "model")])
mod_list <- Map(left_join, mod_list, mod_list_padj, by = "model_term")
return(mod_list)
#compute a confidence interval using non-parametric bootstrapping
if (calc_CI == TRUE){
#extract model term names to use in CI_list
term_names <- levels(as.factor(mod_list$`1`$term))
#iterate bootstrap function across list, calculate and extract CI
CI_list <- lapply(seqmat_integrals_list, boot, compute_CI, R = boot_k)
#this function gets bca confidence intervals for all model terms including the intercept
getCI <- function(x,i) {
ci <- boot.ci(x,index=i)
# extract info for bca
ci <- t(sapply(ci["bca"],function(x) tail(c(x),2)))
# combine with metadata: CI method, index
ci_df <- cbind(i,rownames(ci),as.data.frame(ci))
colnames(ci_df) <- c("term","method","lower","upper")
ci_df
}
#suppressWarnings is used because the model wasn't a t distribution so getCI does not need to be supplied with
#bootstrap variances
CI_list <- suppressWarnings(lapply(CI_list, function(x) do.call(rbind,lapply(1:length(levels(as.factor(mod_list[[1]]$term))),getCI,x = x))))
#create list of OTU or cluster names to eventually join the CI list to the model list
names_df <- data.frame(names(CI_list))
colnames(names_df)[1] <- name_variable
names_list <- split(names_df, f = as.factor(names_df[,name_variable]))
#combine names to CI list dataframes and then melt into longform. Then rename columns in each dataframe
CI_list <- Map(f = cbind, CI_list, names_list)
CI_list <- do.call("rbind", CI_list)
CI_list$term <- term_names
CI_list$term_cluster <- paste(CI_list$term, "_", CI_list[,name_variable])
CI_list <- CI_list[,!names(CI_list) %in% c("cluster","term")]
#join the model list and CI list by OTU/cluster names
mod_list <- do.call("rbind", mod_list)
mod_list$term_cluster <- paste(mod_list$term, "_", mod_list[,name_variable])
#join CI_list to mod_list
mod_df <- left_join(mod_list, CI_list, by = "term_cluster")
mod_df <- mod_df[,!names(mod_df) %in% c("term_cluster", "model_term")]
} else if (calc_CI == FALSE){
#row bind all of the data.frames into a single tidy dataframe
mod_df <- do.call("rbind", mod_list)
}
#clean up the term column using regex to remove words/ parenthesis
mod_df$term <- gsub(mod_df$term, pattern = "\\(", replacement = "")
mod_df$term <- gsub(mod_df$term, pattern = "\\)", replacement = "")
mod_df$term <- gsub(mod_df$term, pattern = "independent_var", replacement = "")
mod_df$name_variable <- mod_df[,name_variable]
#create term + name_variable column to left join means to mod_df
mod_df$term_namevar <- paste0(mod_df$term, "", mod_df$name_variable)
#calculate base mean, term means, and fold changes where appropriate
seqmat_integrals <- do.call("rbind", seqmat_integrals_list)
if (name_variable == "cluster"){
intercept_mean <- seqmat_integrals %>%
group_by(cluster) %>%
summarise(group_means = mean(value))
} else if (name_variable == "OTU"){
intercept_mean <- seqmat_integrals %>%
group_by(OTU) %>%
summarise(group_means = mean(value))
}
intercept_mean$term <- "Intercept"
colnames(intercept_mean)[colnames(intercept_mean) == name_variable] <- "name_variable"
intercept_mean$term_namevar <- paste0(intercept_mean$term, "", intercept_mean$name_variable)
#join intercept mean to mod_df
mod_df$intercept_means <- intercept_mean$group_means[match(mod_df$term_namevar, intercept_mean$term_namevar)]
if (!is.null(groups)){
#set base mean variable by taking the first element of the groups vector
base_group <- groups[1]
comparison_group <- groups[-1]
mod_df$base_group <- base_group
if (name_variable == "cluster"){
group_means <- seqmat_integrals %>%
group_by(cluster, independent_var) %>%
summarise(group_means = mean(value))
} else if (name_variable == "OTU"){
group_means <- seqmat_integrals %>%
group_by(OTU, independent_var) %>%
summarise(group_means = mean(value))
}
base_means <- subset(group_means, independent_var==base_group)
colnames(base_means)[colnames(base_means) == name_variable] <- "name_variable"
group_means <- subset(group_means, independent_var %in% comparison_group)
colnames(group_means)[colnames(group_means) == name_variable] <- "name_variable"
group_means$term_namevar <- paste0(group_means$independent_var, "", group_means$name_variable)
mod_df$base_mean <- base_means$group_means[match(mod_df$name_variable, base_means$name_variable)]
mod_df$term_means <- group_means$group_means[match(mod_df$term_namevar, group_means$term_namevar)]
mod_df$term_means[is.na(mod_df$term_means)] <- mod_df$intercept_means[!is.na(mod_df$intercept_means)]
mod_df <- mod_df[, !names(mod_df) %in% "intercept_means"]
}
mod_df <- mod_df[,!names(mod_df) %in% c("term_namevar", "name_variable")]
return(mod_df)
}
compare_pd(asv_list = asv_list_clean, groups = c("C", "D"))
compare_pd(asv_list = asv_list_clean)
compare_pd(asv_list = asv_list, groups = c("Dirty-A", "Clean-A"))
library(devtools)
install()
library(ASVclustr)
build()
install()
library(ASVclustr)
library(devtools)
build()
install()
library(ASVclustr)
remove.packages(ASVclustr)
install("")
install()
check()
setwd("C:/Users/crtif/Desktop/ASVclustr")
setwd("C:/Users/crtif/Desktop/ASVclustr-master")
install()
library(ASVclustr)
setwd("C:/Users/crtif/Desktop/ASVclustr/ASVclustr")
install()
library(roxygen2)
install()
install()
install()
setwd("C:/Users/crtif/Desktop/ASVclustr-master")
install()
library(ASVclustr)
taxa_by_cluster <- function(melted_asv_list, tax_level, time_var, sam_var, plot_boxplot = FALSE){
#rename variables of interest
colnames(melted_asv_list)[colnames(melted_asv_list) == tax_level] <- "tax_level"
colnames(melted_asv_list)[colnames(melted_asv_list) == sam_var] <- "sample"
colnames(melted_asv_list)[colnames(melted_asv_list) == time_var] <- "timepoint"
#group data.frame by variables, then sum abundance by these variables
melted_asv_list <- melted_asv_list %>%
group_by(tax_level, sample, timepoint, cluster) %>%
summarise(taxa_cluster_sum = sum(Abundance))
#return a new dataframe
if (plot_boxplot == FALSE){
return(melted_asv_list)
#or return a plot of choice if plot_geom parameter is used
} else if (plot_boxplot == TRUE){
ggplot(data = melted_asv_list,
aes(x = timepoint, y = taxa_cluster_sum, fill = tax_level, color = tax_level)) +
geom_boxplot(alpha = 1/4, size = 1, outlier.alpha = 1) + facet_wrap(.~cluster, scales = "free")
}
}
make_asv_list <- function(seqmat, taxmat, meta){
if (!is.matrix(seqmat) && !is.numeric(seqmat)){
stop("seqmat must be a numeric matrix")
}
if (!is.matrix(taxmat) && !is.character(taxmat)){
stop("taxmat must be a character matrix")
}
if (!is.data.frame(meta) && !identical(rownames(seqmat), rownames(meta))){
stop("meta must be a data.frame with identical rownames to seqmat")
}
#Normalize read counts
#calculate the geometric mean for each ASV
gm_mean = function(x, na.rm=TRUE){
exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geomeans <- sapply(as.data.frame(seqmat), gm_mean)
#divide sample ASV abundance by geomean
seqmat_t <- t(seqmat)
seqmat_r <- as.data.frame(seqmat_t/geomeans)
#calculate normalization factor for each sample
seqmat_r[seqmat_r == 0] <- NA
normalization_factors <- sapply(seqmat_r, median, na.rm = TRUE)
#divide each ASV in each sample by sample normalization factor
seqmat_norm <- t(sweep(x = seqmat_t, MARGIN = 2, normalization_factors,  FUN = "/"))
asv_list <- list(seqmat = seqmat, taxmat = taxmat, meta = meta, seqmat_norm = seqmat_norm)
class(asv_list) <- append(class(asv_list), "ASVclustr")
return(asv_list)
}
